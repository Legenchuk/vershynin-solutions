\documentclass[11pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,mathtools,bbm}
\usepackage{enumitem}
\usepackage{xcolor}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{solution}[1][Solution]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep]}{\end{trivlist}} 

\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\LVert}{\left{||}}
\newcommand*{\annot}[1]{\tag*{\footnotesize{(#1)}}}


\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert^2}%

% Swap the definition of \abs* and \norm*, so that \abs
% and \norm resizes the size of the brackets, and the 
% starred version does not.
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
%
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
\title{Chapter 2 Concentration of sums of independent random variables}
\author{Desh Raj} 
 
\maketitle
 
\begin{exercise}{2.1.4}
\end{exercise}
 
\begin{solution}
Given, $g \sim \mathcal{N}(0,1)$. On differentiating, we get $g^{\prime}(t) = -t g(t)$. 

\begin{align*}
\E X^2 \mathbbm{1}_{X>t} &= \int_t^{\infty} x^2 g(x) dx \\
						&= \int_t^{\infty} x (xg(x)) dx \\
						&= x\int_t^{\infty} x g(x) dx + \int_t^{\infty} g(x)dx  \annot{Integration by parts}\\
						&= -xg(x)\bigg]_t^{\infty} + \mathbb{P}(X>t) \\
						&= tg(t) + \mathbb{P}(X>t) \annot{Taking limit of the first term}
\end{align*}
This is the desired result.
\end{solution}

\begin{exercise}{2.2.3}Bounding the hyperbolic cosine
\end{exercise}

\begin{solution}
We have,
\begin{align*}
\text{cosh}(x) &= \frac{e^x + e^{-x}}{2} \\
			&= \frac{1}{2}\left( \sum_{i=0}^{\infty}\frac{x^i}{i!} + \sum_{i=0}^{\infty}\frac{(-x)^i}{i!} \right) \annot{Using Taylor series expansion} \\
			&= \sum_{i=0}^{\infty}\frac{x^{2i}}{(2i)!}
			\geq \sum_{i=0}^{\infty}\frac{x^{2i}}{2^i i!} \annot{Consider the denominator of the $i^{th}$ term. Each product term in $(2i)!$ is strictly greater than that in $2^i i!$.}\\
			&= \exp\left(\frac{x^2}{2}\right) \annot{Again using Taylor series expansion}.
\end{align*}
\end{solution}

\begin{exercise}{2.2.7}
\end{exercise}

\begin{solution}
Given, $X_1,\ldots,X_N$ are independent random variables such that $X_i \in [m_i,M_i]$. Let $S_N = X_1 + \ldots + X_N$. Then, for $t > 0$, we have

\begin{align*}
\mathbb{P}\{S_N - \E S_N \geq t \} &= \mathbb{P}\{ e^{s(S_N - \E S_N)} \geq e^{st} \} \annot{For some $s>0$} \\
			&\leq \frac{\E e^{s(S_N - \E S_N)}}{e^{st}} \annot{Using Markov's inequality} \\
			&= e^{-st}\prod_{i=1}^N \E\left[ e^{s(X_i - \E X_i)} \right] \\
			&\leq e^{-st}\prod_{i=1}^N \exp \left( \frac{s^2(M_i-m_i)^2}{8} \right)  \annot{Using Hoeffding's lemma, since $\E (X_i - \E X_i) = 0$}\\	
			&= \exp \left( -st + \frac{s^2}{8}\sum_{i=1}^N (M_i - m_i)^2 \right).		
\end{align*}

Let $g(s) = \exp \left( -st + \frac{s^2}{8}\sum_{i=1}^N (M_i - m_i)^2 \right)$. $g(s)$ achieves its minimum at $s = \frac{4t}{\sum_{i=1}^N (M_i - m_i)^2}$. Substituting this value in the inequality, we get
\begin{equation*}
\mathbb{P}\{ S_N - \E S_N \geq t \} \leq \exp \left(- \frac{2t^2}{\sum_{i=1}^N (M_i - m_i)^2} \right).
\end{equation*}
\end{solution}

\begin{exercise}{2.2.8}
\end{exercise}

\begin{solution}
Let $X_i$ denote the Bernoulli random variable indicating that the answer is wrong in the $i^{th}$ turn. Then $\E X_i = \frac{1}{2} - \delta$, and $X_i \in [0,1]$. We have to bound the probability that the final answer is wrong with probability $\epsilon$, i.e., $\mathcal{P}$(majority of decisions are wrong) $\leq \epsilon$. From Hoeffding's inequality, we have

\begin{align*}
& \mathcal{P}\left( \sum_{i=1}^N \left( X_i - \E X_i \right) \geq t \right) \leq \exp \left( \frac{-2t^2}{\sum_{i=1}^N (M_i - m_i)^2} \right) \\
\Rightarrow & \mathcal{P}\left( \sum_{i=1}^N \left( X_i - \frac{1}{2} + \delta \right) \geq t \right) \leq \exp \left( \frac{-2t^2}{N} \right) \annot{Since $M_i = 1$ and $m_i = 0$} \\
\end{align*}

To get a wrong answer finally, we require that at least half of all answers must be wrong, i.e. 
\begin{align*}
& \sum_{i=1}^N X_i \geq \frac{N}{2} \\
\Rightarrow & \sum_{i=1}^N \left( X_i - \frac{1}{2} + \delta \right) \geq \delta N \\ 
\end{align*}

Plugging $t = \delta N$ in earlier inequality, we get
\begin{align*}
& \mathcal{P}\left( \sum_{i=1}^N \left( X_i - \frac{1}{2} + \delta \right) \geq \delta N \right) \leq \exp \left( -2N \delta^2 \right)\\
\end{align*}

For this probbility to be bounded by $\epsilon$, we require that
\begin{align*}
& \exp(-2 N \delta^2) \leq \epsilon \\
\Rightarrow & -2 N \delta^2 \leq \log (\epsilon) \annot{Taking log both sides} \\
\Rightarrow & N \geq \frac{\log(\epsilon^{-1})}{2\delta^2}.\\
\end{align*}
\end{solution}

\begin{exercise}{2.2.9}
\end{exercise}

\begin{solution}
\textbf{Part 1}

We want to bound the probability that the sample mean deviates from the true mean by a quantity greater than $\epsilon$. Using Chebyshev's inequality, we have
\begin{equation}
\mathcal{P}\left( \lVert \frac{1}{N}\sum_{i=1}^N X_i - \mu \rVert \geq \epsilon \right) \leq \frac{\text{var}\left( \frac{1}{N}\sum_{i=1}^N X_i \right)}{\epsilon^2}
\label{eqn:1}
\end{equation}

Since the samples are drawn i.i.d, we have
\begin{align*}
& \text{var}(X_1 + \ldots + X_N) = \text{var}(X_1) + \ldots + \text{var}(X_N) \\
\Rightarrow & \text{var}(X_1 + \ldots + X_N) = N\sigma^2 \annot{Since var($X_i$)=$\sigma^2$} \\
\Rightarrow & \frac{1}{N^2}\text{var}(X_1 + \ldots + X_N) = \frac{\sigma^2}{N} \annot{Dividing both sides by $\sigma^2$} \\
\Rightarrow \text{var}\left( \frac{1}{N}\sum_{i=1}^N X_i \right) = \frac{\sigma^2}{N} \\
\end{align*}

Plugging this value in \ref{eqn:1}, we get

\begin{equation*}
\mathcal{P}\left( \lVert \frac{1}{N}\sum_{i=1}^N X_i - \mu \rVert \geq \epsilon \right) \leq \frac{\sigma^2}{N\epsilon^2}
\end{equation*}

We want that an $\epsilon$-accurate estimate should be computed with probability at least 3/4, i.e., the error probability should be at most 1/4. Using this bound, we get $\frac{\sigma^2}{N\epsilon^2} \leq \frac{1}{4}$, which implies $N = \mathcal{O}(\frac{\sigma^2}{\epsilon^2})$.

\textbf{Part 2}

Using Theorem 2.2.6 with $X_i \in [-\sigma,\sigma]$, we get
\begin{align*}
\mathcal{P}\left( \frac{1}{N}\sum_{i=1}^N X_i - \mu \geq \epsilon \right) &= \mathcal{P}\left( \sum_{i=1}^N (X_i - \mu) \geq N\epsilon \right) \annot{Multiplying both sides by $N$}\\
	&\leq \exp \left( -\frac{2N^2\epsilon^2}{\sum_{i=1}^N(2\sigma)^2} \right) \annot{From Theorem 2.2.6} \\
	&= \exp \left( -\frac{2N^2\epsilon^2}{4N\sigma^2} \right) \\
	&= \exp \left( -\frac{N\epsilon^2}{2\sigma^2} \right) \\
\end{align*}

We want that an $\epsilon$-accurate estimate should be computed with probability at least $1-\delta$, i.e., the error probability should be at most $\delta$. This means that
\begin{align*}
&\exp \left( -\frac{N\epsilon^2}{2\sigma^2} \right) \leq \delta \\
\Rightarrow & -\frac{N\epsilon^2}{2\sigma^2} \leq \log (\delta) \annot{Taking log both sides}\\
\Rightarrow & N = \mathcal{O}(\log(\delta^{-1})\frac{\sigma^2}{\epsilon^2}),
\end{align*}
which is the desired result.
\end{solution}

\begin{exercise}{2.2.10}
\end{exercise}

\begin{solution}
\textbf{Part 1}

Let $f_{X_i}$ denote the probability density function. Since $X_i$ is non-negative, it's MGF is given as

\begin{align*}
M_{X_i}(-t) &= \int_{0}^{\infty} e^{-tx}f_{X_i}(x) dx \\
\end{align*}

Since $e^{-tx} \geq 0$, $e^{-tx}f_{X_i} \leq e^{-tx}\text{max}\lVert f_{X_i}\rVert$ = $e^{-tx}$ (since $\text{max}\lVert f_{X_i}\rVert = 1$). Using this in above equation, we get

\begin{align*}
M_{X_i}(-t) &\leq \int_{0}^{\infty} e^{-tx} dx = \frac{1}{t}\\
\end{align*}

\textbf{Part 2}

We have
\begin{align*}
\mathcal{P}\left( \sum_{i=1}^N X_i \leq \epsilon N \right) &= \mathcal{P}\left( \sum_{i=1}^N -X_i \geq -\epsilon N \right) \\
	&=  \mathcal{P}\left( \sum_{i=1}^N -\frac{X_i}{\epsilon} \geq -N \right) \\
	&= \mathcal{P}\left( \exp(\sum_{i=1}^N -\frac{X_i}{\epsilon} \geq \exp(-N) \right) \annot{Since exp is monotonously increasing}\\
	&\leq e^N \E \exp \left( \sum_{i=1}^N -\frac{X_i}{\epsilon} \right) \annot{Using Markov inequality}\\
	&= e^n \prod_{i=1}^N \E \exp \left( -\frac{X_i}{\epsilon} \right) \\
	&\leq e^N \prod_{i=1}^N \epsilon \annot{Using result in part 1} \\
	&= (e\epsilon)^N.
\end{align*}
\end{solution}

\begin{exercise}{2.3.2}
\end{exercise}

\begin{solution}
Since $t<\mu$, so the function $f(x) = \left(\frac{t}{\mu}\right)^x$ is monotonically decreasing. So we can write

\begin{align*}
\mathcal{P}( S_N \leq t ) &= \mathcal{P}\left( \left( \frac{t}{\mu} \right)^{S_N} \geq \left( \frac{t}{\mu} \right)^t \right) \\
	&\leq \frac{\E \left( \frac{t}{\mu} \right)^{S_N}}{\left( \frac{t}{\mu} \right)^t} \annot{By Markov's inequality} \\
\end{align*}

We have
\begin{align*}
\E \alpha^{S_N} = \E \alpha^{\sum_{i=1}^N X_i} = \E \prod_{i=1}^N \alpha^{X_i} \\
				&= \prod_{i=1}^N \left[ p_i \alpha^1 + (1-p_i)\alpha^0 \right] \annot{Since $X_i$ can only take values 1 and 0 with probability $p_i$ and $1-p_i$} \\
				&= \prod_{i=1}^N (1 + (\alpha - 1)p_i) \\
				&\leq \prod_{i=1}^N \exp ((\alpha - 1)p_i) \annot{Using $1+x \leq e^x$} \\
				&= \exp(\sum_{i=1}^N ((\alpha - 1)p_i)) \\
				&= \exp[(\alpha - 1)\mu] \\
\end{align*}

Using this inequality in above, we get
\begin{align*}
\mathcal{P}( S_N \leq t ) &\leq \exp(t-\mu)\left( \frac{\mu}{t}\right)^t \\
				&= e^{-\mu}\left(\frac{e\mu}{t} \right)^t,
\end{align*}
which is the desired result.
\end{solution}

\begin{exercise}{2.3.3}
\end{exercise}

\begin{solution}
Since $X \sim \text{Pois}(\lambda)$, by Poisson limit theorem, we can say that $X$ is the sum of $N$ Bernoulli random variables $X_1, \ldots, X_N$, for some large $N$, where $\E X_i = \lambda$. As such, we can use the Chernoff concentration bound for the sum of Bernoulli random variables to bound $X$, hence giving the desired result.
\end{solution}

\begin{exercise}{2.3.5}
\end{exercise}

\begin{solution}
First, using $t=(1+\delta)\mu$ in Chernoff bounds for upper tails, we get
\begin{equation}
\mathcal{P}\{ X-\mu \geq \delta\mu \} \leq e^{-\mu}\left( \frac{e}{1+\delta} \right)^{(1+\delta)\mu}.
\label{eqn:2}
\end{equation}
Now, using $t=(1-\delta)\mu$ in Chernoff bounds for lower tails (proved in previous problem), we get
\begin{equation}
\mathcal{P}\{ X-\mu \leq -\delta\mu \} \leq e^{-\mu}\left( \frac{e}{1-\delta} \right)^{(1-\delta)\mu}.
\label{eqn:3}
\end{equation}

Adding \ref{eqn:2} and \ref{eqn:3}, we get
\begin{align*}
\mathcal{P}\{ \lVert X-\mu \rVert \geq \delta\mu \} &\leq e^{-\mu}\left[ \left( \frac{e}{1+\delta} \right)^{(1+\delta)\mu} + \left( \frac{e}{1-\delta} \right)^{(1-\delta)\mu}\right] \\
\end{align*}

We can bound the terms inside the bracket on the RHS as follows.
\begin{align*}
\left( \frac{e}{1+\delta} \right)^{(1+\delta)\mu} &= \exp \left[ (1+\delta)\mu \log \left( \frac{e}{1+\delta} \right) \right] \\ 
	&= \exp \left[ \mu(1+\delta)(1 - \log(1+\delta)) \right] \\
	&\leq \exp \left[ \mu(1+\delta)(1 - \frac{\delta}{1+\delta/2}) \right] \annot{Since $\log(1+x) \geq \frac{x}{1+x/2}$}\\
	&= \exp \left[ \mu(1+\delta)\frac{1 - \frac{\delta}{2}}{1 + \frac{\delta}{2}} \right] \\
	&= \exp \left[ \mu\left(1 - \frac{\delta^2}{2+\delta} \right) \right] \\
	&\leq  \exp \left[ \mu\left(1 - \frac{\delta^2}{3} \right) \right] \annot{Since $2+\delta \leq 3$}\\
\end{align*}
and
\begin{align*}
\left( \frac{e}{1-\delta} \right)^{(1-\delta)\mu} &= \exp \left[ (1-\delta)\mu \log \left( \frac{e}{1-\delta} \right) \right] \\ 
	&= \exp \left[ \mu(1-\delta)(1 - \log(1-\delta)) \right] \\
	&\leq \exp \left( \mu(1-\delta)(1 - \frac{-\delta}{\sqrt{1-\delta}}) \right] \annot{Since $\log(1-x) \geq \frac{-x}{\sqrt{1-x}}$}\\
	&= \exp \left[ \mu (1 - \delta + \delta\sqrt{1-\delta}) \right] \\
	&\leq \exp \left[ \mu\left(1 - \frac{\delta^2}{3} \right) \right] \annot{Since $\sqrt{1-\delta}\leq 1- \frac{\delta}{2}$}\\
	\end{align*}

Using these bounds, we get
\begin{align*}
\mathcal{P}\{ \lVert X-\mu \rVert \geq \delta\mu \} &\leq 2\exp\left[-\mu\left(1 - \frac{\delta^2}{3} \right)\right] \\
	&\leq 2e^{-c\mu \delta^2}.
\end{align*}

\end{solution}

\begin{exercise}{2.3.6}
\end{exercise}

\begin{solution}
Since $X \sim \text{Pois}(\lambda)$, by Poisson limit theorem, we can say that $X$ is the sum of $N$ Bernoulli random variables $X_1, \ldots, X_N$, for some large $N$, where $\E X_i = \lambda$. Using the result in Exercise 2.3.5, we get
\begin{equation}
\mathcal{P}\{ \lVert X - \lambda \rVert \geq \delta\lambda \} \leq 2\exp(-c\lambda\delta^2).
\label{eqn:4}
\end{equation}

Now substituting $t=\delta \lambda$ in \ref{eqn:4}, we get the desired result.
\end{solution}

\begin{exercise}{2.3.8}
\end{exercise}

\begin{solution}
We will first show that the sum of independent Poisson distributions is a Poisson distribution. Let $X$ and $Y$ be Poisson distributions with means $\lambda_1$ and $\lambda_2$, respectively. Their MGFs are given as $M_X(s) = e^{-\lambda_1(1-s)}$ and $M_Y(s) = e^{-\lambda_2(1-s)}$. Then, we have
\begin{equation*}
M_{X+Y}(s) = M_X(s)M_Y(s) = e^{-(\lambda_1 + \lambda_2)(1-s)}.
\end{equation*}
Hence, $X+Y \sim \text{Pois}(\lambda_1 + \lambda_2)$. 

It is given that $X \sim \text{Pois}(\lambda)$. Therefore, $X$ can be considered as a sum of $\lambda$ Poisson distributions with mean $1$, using the above result. Now consider the distribution $Z = \frac{X - \lambda}{\sqrt{\lambda}}$. We can directly apply the Central Limit Theorem to $Z$ to obtain the desired result, since $\E X = \lambda$ and $\text{var}(X) = \lambda$.  
\end{solution}

\begin{exercise}{2.4.2}
\end{exercise}

\begin{solution}
Consider a fixed vertex $i$. Using Markov inequality, we can bound the probability that its degree $d_i$ is larger than $\mathcal{O}(\log n)$, as
\begin{equation*}
\mathcal{P}\{ d_i \geq \mathcal{O}(\log n) \} \leq \frac{\E d_i}{\mathcal{O}(\log n)} = c \\
\end{equation*}

We can now use the union bound to bound the probability that such a vertex exists as follows
\begin{equation*}
\mathcal{P}\{\exists i : d_i \geq \mathcal{O}(\log n) \} = \sum_{i=1}^n\mathcal{P}\{d_i \geq \mathcal{O}(\log n) \}  \leq cn  \\
\end{equation*}

We can choose a constant $c$ such that this probability is very low, and therefore, the probability of its counterpart is very high.
\end{solution}

\begin{exercise}{2.4.3}
\end{exercise}

\begin{solution}
Similar to the previous exercise, we can show that if the expected degree is $d = \mathcal{O}(1)$, then with a high probability, all vertices of $G$ have degrees $\mathcal{O}(1)$. Furthermore, we know from asymptotic notations that $\mathcal{O}(1) \leq \mathcal{O}\left(\frac{\log n}{\log \log n}\right)$. Hence, the result is satisfied.
\end{solution}

\begin{exercise}{2.4.4}
\end{exercise}

\begin{solution}
Since degrees are not independent, let us define $d_i^{\prime}$ as the out-degree of the $i^{th}$ vertex of the corresponding directed graph. Since we know that degrees are always larger than out-degrees, if we can show that with high probability, there exists an $i$ such that $d_i^{\prime} = 10d$, then we are done. For this, we can assume that in the limit of a large $n$, the degree distribution (which is actually a Binomial) approximates a Poisson distribution. With this assumption, we can now use the Poisson tail bound for an exact value (2.9).

\textit{Note: I have not been able to derive the exact result for this problem.}
\end{solution}

\begin{exercise}{2.4.5}
\end{exercise}

\begin{solution}
Since the degrees approximately follow the Poisson distribution for large $n$, and given that the expected degrees are $\mathcal{O}(1)$ we have
\begin{equation}
\mathcal{P}\{ d_i = k \} = e^{-1}\frac{1^k}{k!} \geq \frac{e^{-1}}{k^k}, 
\label{eqn:5}
\end{equation}
where the last inequality is obtained by Stirling's approximation.

For the given $k$, we have
\begin{align*}
\log k^k &= k \log k \\
	&= \frac{\log n}{\log \log n}(\log \log n - \log \log \log n) \\
	&\approx \log n \annot{Since $\log \log n >> \log \log \log n$}\\
\therefore k^k &\approx n
\end{align*}

Using this approximation in \ref{eqn:5} and taking union bound over all vertices, we see that the required probability is at least $e^{-1}$, which is sufficiently large.
\end{solution}

\begin{exercise}{2.5.1}
\end{exercise}

\begin{solution}
We have
\begin{align*}
\E X^p &= \int_{-\infty}^{\infty}x^p \frac{1}{\sqrt{2\pi}}e^{-x^2/2}dx \\
	&= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}x^{p-1} (xe^{-x^2/2})dx \\
	&= \frac{1}{\sqrt{2\pi}}\left[ -x^{p-1}e^{-x^2/2}|_{-\infty}^{\infty} + (p-1)\int_{-\infty}^{\infty}x^{p-2}xe^{-x^2/2}dx \right] \annot{Integration by parts} \\
	&= \frac{p-1}{\sqrt{2\pi}} \int_{-\infty}^{\infty}x^{p-2}xe^{-x^2/2}dx \annot{Since the first term is 0} \\
	&= (p-1)\E X^{p-2} \\
	&= (p-1)(p-3)\ldots(3)(1) \annot{Since $\E X^0 = 1$} \\
	&= \frac{p!}{2.4\ldots p} \\
	&= \frac{p!}{2^{p/2}(\frac{p}{2})!} \\
\therefore \lVert X \rVert_p = (\E X^p)^{1/p} &= \frac{1}{\sqrt{2}}\left(\frac{p!}{(p/2)!} \right)^{1/p}, \\
\end{align*}
and the equivalence to the desired result can be seen by using the definition $\Gamma(z) = (z-1)!$.

We can further write
\begin{align*}
\lVert X \rVert_p = (\E X^p)^{1/p} &= \frac{1}{\sqrt{2}}\left(\frac{p!}{(p/2)!} \right)^{1/p} \\
		&= \frac{1}{\sqrt{2}}\left(p(p-1)\ldots(\frac{p}{2}+1) \right)^{1/p} \\
		&\leq \frac{1}{\sqrt{2}}\left(p^{p/2} \right)^{1/p} \annot{Since each term in product is less than $p$}\\
		&= \mathcal{O}(\sqrt{p})
\end{align*}
as $p \rightarrow \infty$.

For the moment-generating function, we have
\begin{align*}
\E \exp(\lambda X) &= \int_{-\infty}^{\infty}e^{\lambda x}\frac{e^{-x^2/2}}{\sqrt{2\pi}}dx \\
	&= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{1}{2}x^2 + \lambda x}dx \\
	&= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{1}{2}(x^2 - 2\lambda x + \lambda^2) + \frac{1}{2}\lambda^2}dx \annot{Adding and subtracting $\frac{1}{2}\lambda^2$} \\
	&= \frac{e^{\lambda^2/2}}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{(x-\lambda)^2}{2}}dx \\
	&= e^{\lambda^2/2}\mathcal{N}(\lambda,1) \\
	&= e^{\lambda^2/2},
\end{align*}
which is the desired result.
\end{solution}

\begin{exercise}{2.5.4}
\end{exercise}

\begin{solution}
\begin{align*}
\E \exp(\lambda^2 X^2) &= \E \exp (\lambda X)^2 \\
		&= \E exp \sum_{i=1}^{\lambda X}\lambda X \\
		&= \E \prod_{i=1}^{\lambda X}\exp(\lambda X) \\
		&\leq \E\prod_{i=1}^{\lambda X}\exp (\lambda^2) \annot{Using Property 5 with $K_5 = 1$}\\
		&= \lambda^3 \E X \leq \exp(K^2 \lambda^2) \annot{Using Property 4}\\
\end{align*}

This inequality can only hold for all $\lambda \in \mathcal{R}$ if $\E X = 0$.
\end{solution}


\begin{exercise}{2.5.5}
\end{exercise}

\begin{solution}

\textbf{Part 1}

We can calculate the MGF of $X^2$ for $X \sim \mathcal{N}(0,1)$ as follows.
\begin{align*}
\E \lambda X^2 &= \int_{-\infty}^{\infty}\lambda x^2 \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx \\
		&= \frac{2\lambda}{\sqrt{2\pi}}\int_{0}^{\infty}x(xe^{-\frac{x^2}{2}})dx \\
		&= \frac{2\lambda}{\sqrt{2\pi}}\left[ -e^{-\frac{x^2}{2}}|_0^{\infty} + \int_{0}^{\infty}e^{-\frac{x^2}{2}}dx \right] \annot{Integration by parts}\\
		&= \lambda\left( \frac{\sqrt{2}}{\pi} + 1 \right)\\
\end{align*}
This is a monotonously increasing function in $\lambda$, and therefore it is only finite in a bounded neighborhood of $0$.

\textbf{Part 2}

We have
\begin{align*}
& \E \exp(\lambda^2 X^2) \leq \exp(K \lambda^2) \\
\Rightarrow & \E \left( 1 + \frac{\lambda^2X^2}{1!} + \frac{(\lambda^2X^2)^2}{2!} + \ldots  \right) \leq 1 + \frac{K\lambda^2}{1!} + \frac{(K\lambda^2)^2}{2!} + \ldots \annot{Using Taylor expansion}\\
\therefore & \forall i \in {0,1,\ldots},~~~\lambda^{2i}\E X^{2i} \leq \lambda^{2i}(\sqrt{K})^{2i} \annot{Comparing individual terms since all terms are positive}\\
\therefore & \E X^{2i} \leq K^i \\  
\Rightarrow & \E ((X^2)^i)\leq K \\
\therefore \lVert X \rVert_{\infty} \leq K
\end{align*}
\end{solution}

\begin{exercise}{2.5.7}
\end{exercise}

\begin{solution}
To prove that a function is a norm, it needs to satisfy the following 3 properties:
\begin{enumerate}
\item $p(\mathbf{u}+\mathbf{v})\leq p(\mathbf{u})+p(\mathbf{v})$
\item $p(a\mathbf{v}) = |a|p(\mathbf{v})$
\item If $p(\mathbf{v})=0$, then $\mathbf{v}=0$
\end{enumerate}

We will now show each of these properties for the sub-gaussian norm $\lVert \cdot \rVert_{\psi_2}$.

\begin{enumerate}
\item Let $f(x) = e^{x^2}$. We have
\begin{align*}
f\left( \frac{\vert X+Y \vert}{a+b} \right) &\leq f\left( \frac{\vert X \vert + \vert Y \vert}{a+b} \right) \\
	&\leq \frac{a}{a+b}f\left( \frac{|X|}{a}\right) + \frac{b}{a+b}f\left( \frac{|Y|}{b}\right) \annot{Jensen's inequality for convex functions}\\
\E f\left( \frac{\vert X+Y \vert}{a+b} \right) &\leq \frac{a}{a+b}\E f\left( \frac{|X|}{a}\right) + \frac{b}{a+b}\E f\left( \frac{|Y|}{b}\right) \annot{Taking $\E$ on both sides}\\
\E f\left( \frac{\vert X+Y \vert}{\lVert X \rVert_{\psi_2}+\lVert Y \rVert_{\psi_2}} \right) &\leq \frac{a}{a+b}2 + \frac{b}{a+b}2 = 2 \annot{Taking $a= \lVert X \rVert_{\psi_2}$ and $b = \lVert Y \rVert_{\psi_2}$}\\
\end{align*}

So $\lVert X \rVert_{\psi_2} + \lVert Y \rVert_{\psi_2}$ is in the set ${t>0: \E \exp(X^2/t^2)}\leq 2$, and the proof is complete.

\item We have
\begin{align*}
\lVert aX \rVert_{\psi_2} &= \inf{t>0: \E\exp(-(aX)^2/t^2)\leq 2} \\
	&= \inf {au>0: \E\exp(-X^2/t^2)\leq 2} \annot{Taking $t=au$} \\
	&= a\lVert X \rVert_{\psi_2}
\end{align*}

\item We have 
\begin{align*}
& \lVert X \rVert_{\psi_2} = 0 \\
\Rightarrow & \inf{t>0: \E\exp(-X^2/t^2)\leq 2} = 0 \\
\Rightarrow & \E\exp(-X^2/t^2)\leq 2 \\
\Rightarrow & \exp(-\E X^2/t^2)\leq 2 \annot{By Jensen's inequality} \\
\Rightarrow & \E X^2 \leq -t^2 \log 2 \\
\Rightarrow & \E X^2 \leq \lim_{t\rightarrow0}-t^2 \log 2 \annot{Taking $\lim_{t\rightarrow 0}$ both sides}\\
\Rightarrow & \E X^2 \leq 0 \\
\therefore X = 0
\end{align*}
\end{enumerate}
\end{solution}

\begin{exercise}{2.5.9}
\end{exercise}

\begin{solution}
\begin{enumerate}[label=(\roman*)]
\item $X \sim \text{Pois}(\lambda)$. We have
\begin{align*}
\mathcal{P}\{ X \geq t\} &> \mathcal{P}\{ X = t\} \\
	&= e^{-\lambda}\frac{\lambda^t}{t!} \\ 
\end{align*}
\end{enumerate}
\end{solution}

\begin{exercise}{2.5.10}
\end{exercise}

\begin{solution}

\end{solution}

\begin{exercise}{2.5.11}
\end{exercise}

\begin{solution}

\end{solution}
\end{document}