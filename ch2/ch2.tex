\documentclass[11pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,mathtools,bbm}
\usepackage{xcolor}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{solution}[1][Solution]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep]}{\end{trivlist}} 

\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\LVert}{\left{||}}
\newcommand*{\annot}[1]{\tag*{\footnotesize{(#1)}}}


\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert^2}%

% Swap the definition of \abs* and \norm*, so that \abs
% and \norm resizes the size of the brackets, and the 
% starred version does not.
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
%
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
\title{Chapter 2 Concentration of sums of independent random variables}
\author{Desh Raj} 
 
\maketitle
 
\begin{exercise}{2.1.4}
\end{exercise}
 
\begin{solution}
Given, $g \sim \mathcal{N}(0,1)$. On differentiating, we get $g^{\prime}(t) = -t g(t)$. 

\begin{align*}
\E X^2 \mathbbm{1}_{X>t} &= \int_t^{\infty} x^2 g(x) dx \\
						&= \int_t^{\infty} x (xg(x)) dx \\
						&= x\int_t^{\infty} x g(x) dx + \int_t^{\infty} g(x)dx  \annot{Integration by parts}\\
						&= -xg(x)\bigg]_t^{\infty} + \mathbb{P}(X>t) \\
						&= tg(t) + \mathbb{P}(X>t) \annot{Taking limit of the first term}
\end{align*}
This is the desired result.
\end{solution}

\begin{exercise}{2.2.3}Bounding the hyperbolic cosine
\end{exercise}

\begin{solution}
We have,
\begin{align*}
\text{cosh}(x) &= \frac{e^x + e^{-x}}{2} \\
			&= \frac{1}{2}\left( \sum_{i=0}^{\infty}\frac{x^i}{i!} + \sum_{i=0}^{\infty}\frac{(-x)^i}{i!} \right) \annot{Using Taylor series expansion} \\
			&= \sum_{i=0}^{\infty}\frac{x^{2i}}{(2i)!}
			\geq \sum_{i=0}^{\infty}\frac{x^{2i}}{2^i i!} \annot{Consider the denominator of the $i^{th}$ term. Each product term in $(2i)!$ is strictly greater than that in $2^i i!$.}\\
			&= \exp\left(\frac{x^2}{2}\right) \annot{Again using Taylor series expansion}.
\end{align*}
\end{solution}

\begin{exercise}{2.2.7}
\end{exercise}

\begin{solution}
Given, $X_1,\ldots,X_N$ are independent random variables such that $X_i \in [m_i,M_i]$. Let $S_N = X_1 + \ldots + X_N$. Then, for $t > 0$, we have

\begin{align*}
\mathbb{P}\{S_N - \E S_N \geq t \} &= \mathbb{P}\{ e^{s(S_N - \E S_N)} \geq e^{st} \} \annot{For some $s>0$} \\
			&\leq \frac{\E e^{s(S_N - \E S_N)}}{e^{st}} \annot{Using Markov's inequality} \\
			&= e^{-st}\prod_{i=1}^N \E\left[ e^{s(X_i - \E X_i)} \right] \\
			&\leq e^{-st}\prod_{i=1}^N \exp \left( \frac{s^2(M_i-m_i)^2}{8} \right)  \annot{Using Hoeffding's lemma, since $\E (X_i - \E X_i) = 0$}\\	
			&= \exp \left( -st + \frac{s^2}{8}\sum_{i=1}^N (M_i - m_i)^2 \right).		
\end{align*}

Let $g(s) = \exp \left( -st + \frac{s^2}{8}\sum_{i=1}^N (M_i - m_i)^2 \right)$. $g(s)$ achieves its minimum at $s = \frac{4t}{\sum_{i=1}^N (M_i - m_i)^2}$. Substituting this value in the inequality, we get
\begin{equation*}
\mathbb{P}\{ S_N - \E S_N \geq t \} \leq \exp \left(- \frac{2t^2}{\sum_{i=1}^N (M_i - m_i)^2} \right).
\end{equation*}
\end{solution}

\begin{exercise}{2.2.8}
\end{exercise}

\begin{solution}
Let $X_i$ denote the Bernoulli random variable indicating that the answer is wrong in the $i^{th}$ turn. Then $\E X_i = \frac{1}{2} - \delta$, and $X_i \in [0,1]$. We have to bound the probability that the final answer is wrong with probability $\epsilon$, i.e., $\mathcal{P}$(majority of decisions are wrong) $\leq \epsilon$. From Hoeffding's inequality, we have

\begin{align*}
& \mathcal{P}\left( \sum_{i=1}^N \left( X_i - \E X_i \right) \geq t \right) \leq \exp \left( \frac{-2t^2}{\sum_{i=1}^N (M_i - m_i)^2} \right) \\
\Rightarrow & \mathcal{P}\left( \sum_{i=1}^N \left( X_i - \frac{1}{2} + \delta \right) \geq t \right) \leq \exp \left( \frac{-2t^2}{N} \right) \annot{Since $M_i = 1$ and $m_i = 0$} \\
\end{align*}

To get a wrong answer finally, we require that at least half of all answers must be wrong, i.e. 
\begin{align*}
& \sum_{i=1}^N X_i \geq \frac{N}{2} \\
\Rightarrow & \sum_{i=1}^N \left( X_i - \frac{1}{2} + \delta \right) \geq \delta N \\ 
\end{align*}

Plugging $t = \delta N$ in earlier inequality, we get
\begin{align*}
& \mathcal{P}\left( \sum_{i=1}^N \left( X_i - \frac{1}{2} + \delta \right) \geq \delta N \right) \leq \exp \left( -2N \delta^2 \right)\\
\end{align*}

For this probbility to be bounded by $\epsilon$, we require that
\begin{align*}
& \exp(-2 N \delta^2) \leq \epsilon \\
\Rightarrow & -2 N \delta^2 \leq \log (\epsilon) \annot{Taking log both sides} \\
\Rightarrow & N \geq \frac{\log(\epsilon^{-1})}{2\delta^2}.\\
\end{align*}
\end{solution}

\begin{exercise}{2.2.9}
\end{exercise}

\begin{solution}
\textbf{Part 1}

We want to bound the probability that the sample mean deviates from the true mean by a quantity greater than $\epsilon$. Using Chebyshev's inequality, we have
\begin{equation}
\mathcal{P}\left( \lVert \frac{1}{N}\sum_{i=1}^N X_i - \mu \rVert \geq \epsilon \right) \leq \frac{\text{var}\left( \frac{1}{N}\sum_{i=1}^N X_i \right)}{\epsilon^2}
\label{eqn:1}
\end{equation}

Since the samples are drawn i.i.d, we have
\begin{align*}
& \text{var}(X_1 + \ldots + X_N) = \text{var}(X_1) + \ldots + \text{var}(X_N) \\
\Rightarrow & \text{var}(X_1 + \ldots + X_N) = N\sigma^2 \annot{Since var($X_i$)=$\sigma^2$} \\
\Rightarrow & \frac{1}{N^2}\text{var}(X_1 + \ldots + X_N) = \frac{\sigma^2}{N} \annot{Dividing both sides by $\sigma^2$} \\
\Rightarrow \text{var}\left( \frac{1}{N}\sum_{i=1}^N X_i \right) = \frac{\sigma^2}{N} \\
\end{align*}

Plugging this value in \ref{eqn:1}, we get

\begin{equation*}
\mathcal{P}\left( \lVert \frac{1}{N}\sum_{i=1}^N X_i - \mu \rVert \geq \epsilon \right) \leq \frac{\sigma^2}{N\epsilon^2}
\end{equation*}

We want that an $\epsilon$-accurate estimate should be computed with probability at least 3/4, i.e., the error probability should be at most 1/4. Using this bound, we get $\frac{\sigma^2}{N\epsilon^2} \leq \frac{1}{4}$, which implies $N = \mathcal{O}(\frac{\sigma^2}{\epsilon^2})$.

\textbf{Part 2}

Using Theorem 2.2.6 with $X_i \in [-\sigma,\sigma]$, we get
\begin{align*}
\mathcal{P}\left( \frac{1}{N}\sum_{i=1}^N X_i - \mu \geq \epsilon \right) &= \mathcal{P}\left( \sum_{i=1}^N (X_i - \mu) \geq N\epsilon \right) \annot{Multiplying both sides by $N$}\\
	&\leq \exp \left( -\frac{2N^2\epsilon^2}{\sum_{i=1}^N(2\sigma)^2} \right) \annot{From Theorem 2.2.6} \\
	&= \exp \left( -\frac{2N^2\epsilon^2}{4N\sigma^2} \right) \\
	&= \exp \left( -\frac{N\epsilon^2}{2\sigma^2} \right) \\
\end{align*}

We want that an $\epsilon$-accurate estimate should be computed with probability at least $1-\delta$, i.e., the error probability should be at most $\delta$. This means that
\begin{align*}
&\exp \left( -\frac{N\epsilon^2}{2\sigma^2} \right) \leq \delta \\
\Rightarrow & -\frac{N\epsilon^2}{2\sigma^2} \leq \log (\delta) \annot{Taking log both sides}\\
\Rightarrow & N = \mathcal{O}(\log(\delta^{-1})\frac{\sigma^2}{\epsilon^2}),
\end{align*}
which is the desired result.
\end{solution}

\begin{exercise}{2.2.10}
\end{exercise}

\begin{solution}
\textbf{Part 1}

Let $f_{X_i}$ denote the probability density function. Since $X_i$ is non-negative, it's MGF is given as

\begin{align*}
M_{X_i}(-t) &= \int_{0}^{\infty} e^{-tx}f_{X_i}(x) dx \\
\end{align*}

Since $e^{-tx} \geq 0$, $e^{-tx}f_{X_i} \leq e^{-tx}\text{max}\lVert f_{X_i}\rVert$ = $e^{-tx}$ (since $\text{max}\lVert f_{X_i}\rVert = 1$). Using this in above equation, we get

\begin{align*}
M_{X_i}(-t) &\leq \int_{0}^{\infty} e^{-tx} dx = \frac{1}{t}\\
\end{align*}

\textbf{Part 2}

We have
\begin{align*}
\mathcal{P}\left( \sum_{i=1}^N X_i \leq \epsilon N \right) &= \mathcal{P}\left( \sum_{i=1}^N -X_i \geq -\epsilon N \right) \\
	&=  \mathcal{P}\left( \sum_{i=1}^N -\frac{X_i}{\epsilon} \geq -N \right) \\
	&= \mathcal{P}\left( \exp(\sum_{i=1}^N -\frac{X_i}{\epsilon} \geq \exp(-N) \right) \annot{Since exp is monotonously increasing}\\
	&\leq e^N \E \exp \left( \sum_{i=1}^N -\frac{X_i}{\epsilon} \right) \annot{Using Markov inequality}\\
	&= e^n \prod_{i=1}^N \E \exp \left( -\frac{X_i}{\epsilon} \right) \\
	&\leq e^N \prod_{i=1}^N \epsilon \annot{Using result in part 1} \\
	&= (e\epsilon)^N.
\end{align*}
\end{solution}

\begin{exercise}{2.3.2}
\end{exercise}

\begin{solution}
Since $t<\mu$, so the function $f(x) = \left(\frac{t}{\mu}\right)^x$ is monotonically decreasing. So we can write

\begin{align*}
\mathcal{P}( S_N \leq t ) &= \mathcal{P}\left( \left( \frac{t}{\mu} \right)^{S_N} \geq \left( \frac{t}{\mu} \right)^t \right) \\
	&\leq \frac{\E \left( \frac{t}{\mu} \right)^{S_N}}{\left( \frac{t}{\mu} \right)^t} \annot{By Markov's inequality} \\
\end{align*}

We have
\begin{align*}
\E \alpha^{S_N} = \E \alpha^{\sum_{i=1}^N X_i} = \E \prod_{i=1}^N \alpha^{X_i} \\
				&= \prod_{i=1}^N \left[ p_i \alpha^1 + (1-p_i)\alpha^0 \right] \annot{Since $X_i$ can only take values 1 and 0 with probability $p_i$ and $1-p_i$} \\
				&= \prod_{i=1}^N (1 + (\alpha - 1)p_i) \\
				&\leq \prod_{i=1}^N \exp ((\alpha - 1)p_i) \annot{Using $1+x \leq e^x$} \\
				&= \exp(\sum_{i=1}^N ((\alpha - 1)p_i)) \\
				&= \exp[(\alpha - 1)\mu] \\
\end{align*}

Using this inequality in above, we get
\begin{align*}
\mathcal{P}( S_N \leq t ) &\leq \exp(t-\mu)\left( \frac{\mu}{t}\right)^t \\
				&= e^{-\mu}\left(\frac{e\mu}{t} \right)^t,
\end{align*}
which is the desired result.
\end{solution}

\begin{exercise}{2.3.3}
\end{exercise}

\begin{solution}
Since $X \sim \text{Pois}(\lambda)$, by Poisson limit theorem, we can say that $X$ is the sum of $N$ Bernoulli random variables $X_1, \ldots, X_N$, for some large $N$, where $\E X_i = \lambda$. As such, we can use the Chernoff concentration bound for the sum of Bernoulli random variables to bound $X$, hence giving the desired result.
\end{solution}

\begin{exercise}{2.3.5}
\end{exercise}

\begin{solution}
First, using $t=(1+\delta)\mu$ in Chernoff bounds for upper tails, we get
\begin{equation}
\mathcal{P}\{ X-\mu \geq \delta\mu \} \leq e^{-\mu}\left( \frac{e}{1+\delta} \right)^{(1+\delta)\mu}.
\label{eqn:2}
\end{equation}
Now, using $t=(1-\delta)\mu$ in Chernoff bounds for lower tails (proved in previous problem), we get
\begin{equation}
\mathcal{P}\{ X-\mu \leq -\delta\mu \} \leq e^{-\mu}\left( \frac{e}{1-\delta} \right)^{(1-\delta)\mu}.
\label{eqn:3}
\end{equation}

Adding \ref{eqn:2} and \ref{eqn:3}, we get
\begin{align*}
\mathcal{P}\{ \lVert X-\mu \rVert \geq \delta\mu \} &\leq e^{-\mu}\left[ \left( \frac{e}{1+\delta} \right)^{(1+\delta)\mu} + \left( \frac{e}{1-\delta} \right)^{(1-\delta)\mu}\right] \\
\end{align*}

We can bound the terms inside the bracket on the RHS as follows.
\begin{align*}
\left( \frac{e}{1+\delta} \right)^{(1+\delta)\mu} &= \exp \left[ (1+\delta)\mu \log \left( \frac{e}{1+\delta} \right) \right] \\ 
	&= \exp \left[ \mu(1+\delta)(1 - \log(1+\delta)) \right] \\
	&\leq \exp \left[ \mu(1+\delta)(1 - \frac{\delta}{1+\delta/2}) \right] \annot{Since $\log(1+x) \geq \frac{x}{1+x/2}$}\\
	&= \exp \left[ \mu(1+\delta)\frac{1 - \frac{\delta}{2}}{1 + \frac{\delta}{2}} \right] \\
	&= \exp \left[ \mu\left(1 - \frac{\delta^2}{2+\delta} \right) \right] \\
	&\leq  \exp \left[ \mu\left(1 - \frac{\delta^2}{3} \right) \right] \annot{Since $2+\delta \leq 3$}\\
\end{align*}
and
\begin{align*}
\left( \frac{e}{1-\delta} \right)^{(1-\delta)\mu} &= \exp \left[ (1-\delta)\mu \log \left( \frac{e}{1-\delta} \right) \right] \\ 
	&= \exp \left[ \mu(1-\delta)(1 - \log(1-\delta)) \right] \\
	&\leq \exp \left( \mu(1-\delta)(1 - \frac{-\delta}{\sqrt{1-\delta}}) \right] \annot{Since $\log(1-x) \geq \frac{-x}{\sqrt{1-x}}$}\\
	&= \exp \left[ \mu (1 - \delta + \delta\sqrt{1-\delta}) \right] \\
	&\leq \exp \left[ \mu\left(1 - \frac{\delta^2}{3} \right) \right] \annot{Since $\sqrt{1-\delta}\leq 1- \frac{\delta}{2}$}\\
	\end{align*}

Using these bounds, we get
\begin{align*}
\mathcal{P}\{ \lVert X-\mu \rVert \geq \delta\mu \} &\leq 2\exp\left[-\mu\left(1 - \frac{\delta^2}{3} \right)\right] \\
	&\leq 2e^{-c\mu \delta^2}.
\end{align*}

\end{solution}

\begin{exercise}{2.3.6}
\end{exercise}

\begin{solution}
Since $X \sim \text{Pois}(\lambda)$, by Poisson limit theorem, we can say that $X$ is the sum of $N$ Bernoulli random variables $X_1, \ldots, X_N$, for some large $N$, where $\E X_i = \lambda$. Using the result in Exercise 2.3.5, we get
\begin{equation}
\mathcal{P}\{ \lVert X - \lambda \rVert \geq \delta\lambda \} \leq 2\exp(-c\lambda\delta^2).
\label{eqn:4}
\end{equation}

Now substituting $t=\delta \lambda$ in \ref{eqn:4}, we get the desired result.
\end{solution}

\begin{exercise}{2.3.8}
\end{exercise}

\begin{solution}
We will first show that the sum of independent Poisson distributions is a Poisson distribution. Let $X$ and $Y$ be Poisson distributions with means $\lambda_1$ and $\lambda_2$, respectively. Their MGFs are given as $M_X(s) = e^{-\lambda_1(1-s)}$ and $M_Y(s) = e^{-\lambda_2(1-s)}$. Then, we have
\begin{equation*}
M_{X+Y}(s) = M_X(s)M_Y(s) = e^{-(\lambda_1 + \lambda_2)(1-s)}.
\end{equation*}
Hence, $X+Y \sim \text{Pois}(\lambda_1 + \lambda_2)$. 

It is given that $X \sim \text{Pois}(\lambda)$. Therefore, $X$ can be considered as a sum of $\lambda$ Poisson distributions with mean $1$, using the above result. Now consider the distribution $Z = \frac{X - \lambda}{\sqrt{\lambda}}$. We can directly apply the Central Limit Theorem to $Z$ to obtain the desired result, since $\E X = \lambda$ and $\text{var}(X) = \lambda$.  
\end{solution}

\begin{exercise}{2.4.2}
\end{exercise}

\begin{solution}
Consider a fixed vertex $i$. Using Markov inequality, we can bound the probability that its degree $d_i$ is larger than $\mathcal{O}(\log n)$, as
\begin{equation*}
\mathcal{P}\{ d_i \geq \mathcal{O}(\log n) \} \leq \frac{\E d_i}{\mathcal{O}(\log n)} = c \\
\end{equation*}

We can now use the union bound to bound the probability that such a vertex exists as follows
\begin{equation*}
\mathcal{P}\{\exists i : d_i \geq \mathcal{O}(\log n) \} = \sum_{i=1}^n\mathcal{P}\{d_i \geq \mathcal{O}(\log n) \}  \leq cn  \\
\end{equation*}

We can choose a constant $c$ such that this probability is very low, and therefore, the probability of its counterpart is very high.
\end{solution}

\begin{exercise}{2.4.3}
\end{exercise}

\begin{solution}
Similar to the previous exercise, we can show that if the expected degree is $d = \mathcal{O}(1)$, then with a high probability, all vertices of $G$ have degrees $\mathcal{O}(1)$. Furthermore, we know from asymptotic notations that $\mathcal{O}(1) \leq \mathcal{O}\left(\frac{\log n}{\log \log n}\right)$. Hence, the result is satisfied.
\end{solution}

\begin{exercise}{2.4.4}
\end{exercise}

\begin{solution}

\end{solution}

\begin{exercise}{2.4.5}
\end{exercise}

\begin{solution}

\end{solution}
\end{document}