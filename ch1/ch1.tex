\documentclass[11pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,mathtools,bbm}
\usepackage{xcolor}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{solution}[1][Solution]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep]}{\end{trivlist}} 

\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\LVert}{\left{||}}
\newcommand*{\annot}[1]{\tag*{\footnotesize{(#1)}}}


\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert^2}%

% Swap the definition of \abs* and \norm*, so that \abs
% and \norm resizes the size of the brackets, and the 
% starred version does not.
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
%
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
\title{Chapter 1 Preliminaries on random variables}
\author{Desh Raj} 
 
\maketitle
 
\begin{exercise}{1.2.2}
\end{exercise}
 
\begin{solution}
The derivation is similar to the one for Lemma 1.2.1, which is given in the book. We have,

\begin{align*}
x &= \int_0^x 1 dt - \int_{-x}^0 1 dt \\ 
  &= \int_0^{\infty} \mathbbm{1}_{t<x} dt - \int_{-\infty}^0 \mathbbm{1}_{t>x} dt
\end{align*}

Replacing $x$ with random variable $X$ and taking expectation on both sides, we get
\begin{align*}
\E X &= \int_0^{\infty} \mathbbm{1}_{t<X} dt - \int_{-\infty}^0 \mathbbm{1}_{t>X} dt \\
	 &= \int_0^{\infty} \mathbb{P}(X>t) dt - \int_{-\infty}^0 \mathbb{P}(X<t) dt,
\end{align*}
which concludes our proof.

\end{solution}

\begin{exercise}{1.2.3}
\end{exercise}

\begin{solution}
Since $|X|^p$ is always non-negative, we can use the result in Lemma 1.2.1 to write

\begin{equation}
\E |X|^p = \int_0^{\infty}\mathbb{P}(|X|^p > t)dt.
\label{eqn:1}
\end{equation}

Let $t = z^p$. Then, $dt = pz^{p-1}dz$. Substituting these values in \ref{eqn:1}, we get
\begin{align*}
\E |X|^p &= \int_0^{\infty} \mathbb{P}(|X|^p > z^p) pz^{p-1} dz \\
		&= \int_0^{\infty} \mathbb{P}(|X| > z) pz^{p-1} dz \annot{Using property of exponentiation} \\
		&= \int_0^{\infty} pt^{p-1}\mathbb{P}(|X| > t) dt \annot{Replacing $z$ with $t$},
\end{align*}
which is the desired result.
\end{solution}

\begin{exercise}{1.2.6}Chebyshev's inequality
\end{exercise}

\begin{solution}
Given, $X$ is a random variable with mean $\mu$ and variance $\sigma^2$. We define another random variable $Z$ as $Z = \norm{X-\mu}$. Then, using Markov inequality, we can write
\begin{align*}
\mathbb{P}\{Z \geq t^2\} &\leq \frac{\E Z}{t^2} \\
\mathbb{P}\{\norm{X-\mu} \geq t^2\} &\leq \frac{\E \norm{X-\mu}}{t^2} \annot{Since $Z = \norm{X-\mu}$}\\
\mathbb{P}\{\lVert X-\mu \rVert \geq t\} &\leq \frac{\E \norm{X-\mu}}{t^2} \\
\mathbb{P}\{\lVert X-\mu \rVert \geq t\} &\leq \frac{\sigma^2}{t^2} \annot{Since $\E \norm{X-\mu} = \sigma^2$}\\,
\end{align*}
which is the Chebyshev's inequality.
\end{solution}

\begin{exercise}{1.3.3}
\end{exercise}

\begin{solution}
Given, $X_1, \ldots, X_N$ are a sequence of i.i.d random variables with mean $\mu$ and some finite variance. W.l.o.g we can assume that all the variances are equal (say $\sigma^2$). We define a random variable $Z = \frac{1}{N}\sum_{i=1}^N X_i$. Then, we have 

\begin{equation*}
\E Z = \mu, ~~~~~\text{and}~~~~ \text{var}(Z) = \frac{\sigma^2}{N} \annot{From earlier results}
\end{equation*}

Using the definition of variance, we can then write
\begin{align*}
&\E \norm{Z - \mu} = \frac{\sigma^2}{N} \\
\Rightarrow &\E \abs{Z - \mu} = \frac{\sigma}{\sqrt{N}} \annot{Taking positive square root on both sides} \\
\Rightarrow&\E \abs{\frac{1}{N}\sum_{i=1}^N X_i - \mu} = \frac{\sigma}{\sqrt{N}} = \mathcal{O}(\frac{1}{\sqrt{N}}) \annot{Using definition of $Z$}.
\end{align*}

\end{solution}
\end{document}